{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code to demostrate the following on Hadoop Stream\n",
    "- Word count\n",
    "- Combiners\n",
    "- Sorts, Secondary sorts, total sort (under construction)\n",
    "- Custom partitions\n",
    "\n",
    "### Written by James G. Shanahan\n",
    "MIDS w261 Machine Learning at Scale\n",
    "September 15, 2014\n",
    "\n",
    "### Data used\n",
    "- Selfcontained datesets generated using code\n",
    "- Gutenberg dataset available http://www.gutenberg.org/cache/epub/48054/pg48054.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Anaconda2\\\\Scripts'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkdir WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gutenberg dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1318  100  1318    0     0   4013      0 --:--:-- --:--:-- --:--:--  5491\n"
     ]
    }
   ],
   "source": [
    "#run any unix command\n",
    "!curl http://www.gutenberg.org/cache/epub/48054/pg48054.txt > WordCount/historical_tours.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount Example in map reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following for a detailed presentation of the word count example\n",
    "-     http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n",
    "- See this notebook for more nuances and more intricate extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are you\\n\")\n",
    "\n",
    "long_len = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        if len(word) >= 5:\n",
    "            long_len = 1\n",
    "        else:\n",
    "            long_len = 0\n",
    "        print '%s\\t%s\\t%s\\t%s' % (word, 1, len(word), long_len)\n",
    "        if word == \"debt\":\n",
    "            sys.stderr.write(\"reporter:counter:EDA Counters,Calls,1\\n\")          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "cur_len = 0\n",
    "cur_long = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "\n",
    "    key, value, lgth, lgth_size = line.split()\n",
    "\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s\\t%s\\t%s' % (cur_key, cur_count, cur_len, cur_long)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "        cur_len = lgth\n",
    "        cur_long = lgth_size\n",
    "\n",
    "print '%s\\t%s\\t%s\\t%s' % (cur_key, cur_count, cur_len, cur_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a local test of  your mapper.py and reducer on Unix command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/mapper.py \n",
    "!chmod a+x WordCount/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "foo\t1\t3\t0\r\n",
      "foo\t1\t3\t0\r\n",
      "quux\t1\t4\t0\r\n",
      "sample\t1\t6\t1\r\n",
      "labs\t1\t4\t0\r\n",
      "foo\t1\t3\t0\r\n",
      "bar\t1\t3\t0\r\n",
      "quux\t1\t4\t0\r\n",
      "sample\t1\t6\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux sample labs foo bar quux sample\" | WordCount/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "foo\t3\t3\t0\r\n",
      "quux\t2\t4\t0\r\n",
      "sample\t2\t6\t1\r\n",
      "bar\t1\t3\t0\r\n",
      "labs\t1\t4\t0\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux sample labs foo bar quux sample\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the  Hadoop Cluster HDFS available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay attention to the order in which you specify the opts for hdfs commands\n",
    "\n",
    "Generic options supported are\n",
    "- -conf <configuration file>     specify an application configuration file\n",
    "- -D <property=value>            use value for given property\n",
    "- -fs <local|namenode:port>      specify a namenode\n",
    "- -jt <local|resourcemanager:port>    specify a ResourceManager\n",
    "- -files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
    "- -libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
    "- -archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
    "\n",
    "The general command line syntax is\n",
    "bin/hadoop command [genericOptions] [commandOptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-04 18:07 HW2_2\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-07 03:15 HW2_3\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-03 02:19 Sort\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-13 18:19 WordCount\r\n",
      "-rw-r--r--   1 hadoop hadoop     204678 2016-06-04 18:06 enronemail_1h.txt\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-14 01:59 gutenberg-wordCount\r\n",
      "-rw-r--r--   1 hadoop hadoop       1318 2016-06-14 01:58 historical_tours.txt\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-12 16:11 integers-sorted\r\n",
      "-rw-r--r--   1 hadoop hadoop      98794 2016-06-12 02:10 integers.txt\r\n",
      "-rw-r--r--   1 hadoop hadoop        110 2016-06-14 01:59 ipAddresses.txt\r\n",
      "-rw-r--r--   1 hadoop hadoop         56 2016-06-14 01:58 testWordCountInput.txt\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-10 22:42 tmp\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-14 01:58 wordcount-output\r\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-14 02:03 wordcount-output-sorted\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small test for Word Count (one input file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing testWordCountInput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile testWordCountInput.txt\n",
    "hello this is Jimi\n",
    "jimi who Jimi three Jimi \n",
    "Hello\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/14 02:09:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted testWordCountInput.txt\n",
      "16/06/14 02:09:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar] /tmp/streamjob8127314882734979367.jar tmpDir=null\n",
      "16/06/14 02:09:39 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/14 02:09:39 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/14 02:09:40 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: true maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1464726748890 \n",
      "16/06/14 02:09:40 INFO metrics.MetricsSaver: Created MetricsSaver j-ZAC3GQDMC0E6:i-610a91d4:RunJar:16432 period:60 /mnt/var/em/raw/i-610a91d4_20160614_RunJar_16432_raw.bin\n",
      "16/06/14 02:09:40 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "16/06/14 02:09:40 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 426d94a07125cf9447bb0c2b336cf10b4c254375]\n",
      "16/06/14 02:09:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/14 02:09:40 INFO mapreduce.JobSubmitter: number of splits:19\n",
      "16/06/14 02:09:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464726740139_0089\n",
      "16/06/14 02:09:40 INFO impl.YarnClientImpl: Submitted application application_1464726740139_0089\n",
      "16/06/14 02:09:40 INFO mapreduce.Job: The url to track the job: http://ip-172-31-7-251.us-west-1.compute.internal:20888/proxy/application_1464726740139_0089/\n",
      "16/06/14 02:09:40 INFO mapreduce.Job: Running job: job_1464726740139_0089\n",
      "16/06/14 02:09:49 INFO mapreduce.Job: Job job_1464726740139_0089 running in uber mode : false\n",
      "16/06/14 02:09:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/14 02:09:58 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/06/14 02:10:02 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "16/06/14 02:10:06 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "16/06/14 02:10:07 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "16/06/14 02:10:09 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "16/06/14 02:10:10 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/06/14 02:10:11 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "16/06/14 02:10:13 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "16/06/14 02:10:15 INFO mapreduce.Job:  map 84% reduce 0%\n",
      "16/06/14 02:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/14 02:10:21 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "16/06/14 02:10:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/14 02:10:22 INFO mapreduce.Job: Job job_1464726740139_0089 completed successfully\n",
      "16/06/14 02:10:22 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=142\n",
      "\t\tFILE: Number of bytes written=2844248\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3230\n",
      "\t\tHDFS: Number of bytes written=56\n",
      "\t\tHDFS: Number of read operations=66\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=19\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=11\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11026440\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2772360\n",
      "\t\tTotal time spent by all map tasks (ms)=245032\n",
      "\t\tTotal time spent by all reduce tasks (ms)=30804\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=245032\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=30804\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=352846080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=88715520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=1003\n",
      "\t\tInput split bytes=2679\n",
      "\t\tCombine input records=11\n",
      "\t\tCombine output records=10\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1003\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =57\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=57\n",
      "\t\tGC time elapsed (ms)=3371\n",
      "\t\tCPU time spent (ms)=25390\n",
      "\t\tPhysical memory (bytes) snapshot=9032933376\n",
      "\t\tVirtual memory (bytes) snapshot=48855101440\n",
      "\t\tTotal committed heap usage (bytes)=10444341248\n",
      "\tMapper Counters\n",
      "\t\tCalls=19\n",
      "\tReducer Counters\n",
      "\t\tCalls=11\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=551\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/06/14 02:10:22 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r wordcount-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "#dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar \\\n",
    "   -files WordCount/mapper.py,WordCount/reducer.py \\\n",
    "   -mapper mapper.py \\\n",
    "   -reducer reducer.py \\\n",
    "   -combiner reducer.py \\\n",
    "   -input testWordCountInput.txt \\\n",
    "   -output wordcount-output  \\\n",
    "   -numReduceTasks 3\n",
    "   #--D mapreduce.job.reduces=2  depecated\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n---------------------------\\n\n",
      "hello this is Jimi\n",
      "jimi who Jimi three Jimi \n",
      "Hello\n",
      "hello\\n---------------------------\\n\n",
      "Hello\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "is\t1\n",
      "who\t1\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "!hdfs dfs -cat testWordCountInput.txt\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "# Wordcount output\n",
    "!hdfs dfs -cat wordcount-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the words in descreasing order of frequency, but break ties with alphabetical sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-86-2cf3c4dec7bc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-86-2cf3c4dec7bc>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    I.e., \"-k2,2nr -k1,1\"\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Sort the wordcount out in descreasing order of counts and increasing order of tokens\n",
    "I.e., \"-k2,2nr -k1,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/12 17:50:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output-sorted\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar] /tmp/streamjob5263851993333263738.jar tmpDir=null\n",
      "16/06/12 17:50:30 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/12 17:50:30 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/12 17:50:31 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: true maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1464726748890 \n",
      "16/06/12 17:50:31 INFO metrics.MetricsSaver: Created MetricsSaver j-ZAC3GQDMC0E6:i-610a91d4:RunJar:20372 period:60 /mnt/var/em/raw/i-610a91d4_20160612_RunJar_20372_raw.bin\n",
      "16/06/12 17:50:31 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "16/06/12 17:50:31 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 426d94a07125cf9447bb0c2b336cf10b4c254375]\n",
      "16/06/12 17:50:31 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/06/12 17:50:31 INFO mapreduce.JobSubmitter: number of splits:20\n",
      "16/06/12 17:50:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464726740139_0060\n",
      "16/06/12 17:50:31 INFO impl.YarnClientImpl: Submitted application application_1464726740139_0060\n",
      "16/06/12 17:50:31 INFO mapreduce.Job: The url to track the job: http://ip-172-31-7-251.us-west-1.compute.internal:20888/proxy/application_1464726740139_0060/\n",
      "16/06/12 17:50:31 INFO mapreduce.Job: Running job: job_1464726740139_0060\n",
      "16/06/12 17:50:40 INFO mapreduce.Job: Job job_1464726740139_0060 running in uber mode : false\n",
      "16/06/12 17:50:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/12 17:50:49 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/06/12 17:50:52 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "16/06/12 17:50:55 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/06/12 17:50:57 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/12 17:50:59 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "16/06/12 17:51:02 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "16/06/12 17:51:04 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "16/06/12 17:51:06 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "16/06/12 17:51:08 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "16/06/12 17:51:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/12 17:51:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/12 17:51:11 INFO mapreduce.Job: Job job_1464726740139_0060 completed successfully\n",
      "16/06/12 17:51:11 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=109\n",
      "\t\tFILE: Number of bytes written=2662270\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3167\n",
      "\t\tHDFS: Number of bytes written=75\n",
      "\t\tHDFS: Number of read operations=63\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=20\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=12\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9745065\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1223820\n",
      "\t\tTotal time spent by all map tasks (ms)=216557\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13598\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=216557\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13598\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=311842080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=39162240\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=120\n",
      "\t\tMap output materialized bytes=435\n",
      "\t\tInput split bytes=2920\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=435\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tGC time elapsed (ms)=3143\n",
      "\t\tCPU time spent (ms)=17880\n",
      "\t\tPhysical memory (bytes) snapshot=8852873216\n",
      "\t\tVirtual memory (bytes) snapshot=44322234368\n",
      "\t\tTotal committed heap usage (bytes)=10418126848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=247\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=75\n",
      "16/06/12 17:51:11 INFO streaming.StreamJob: Output directory: wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "# This call to Hadoop does NOT work as anticpated\n",
    "# IdentityMapper, IdentityReducer do not trigger the Hadoop framework to sort properly\n",
    "\n",
    "!hdfs dfs -rm -r wordcount-output-sorted\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -input wordcount-output \\\n",
    "   -output wordcount-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "\n",
    "#DOES not work in streaming mode\n",
    "#   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "#  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\tis\t1\r\n",
      "0\thello\t2\r\n",
      "22\tthree\t1\r\n",
      "0\tHello\t1\r\n",
      "8\tjimi\t1\r\n",
      "15\tthis\t1\r\n",
      "0\tJimi\t3\r\n",
      "13\twho\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount-output-sorted/part-00000| head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 hadoop hadoop          0 2016-06-01 02:07 wordcount-output-sorted/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop         75 2016-06-01 02:07 wordcount-output-sorted/part-00000\n",
      "rm: cannot remove ‘wordcount-output-sorted’: No such file or directory\n",
      "0\tHello\t1\n",
      "0\thello\t2\n",
      "0\tJimi\t3\n",
      "13\twho\t1\n",
      "15\tthis\t1\n",
      "22\tthree\t1\n",
      "8\tis\t1\n",
      "8\tjimi\t1\n"
     ]
    }
   ],
   "source": [
    "#Does NOT work \n",
    "!hdfs dfs -ls wordcount-output-sorted/*\n",
    "!rm -r wordcount-output-sorted\n",
    "!hdfs dfs -copyToLocal wordcount-output-sorted \n",
    "\n",
    "!sort -k2,2nr <wordcount-output-sorted/part-00000 >wordcount-output-sorted/part-00000.SORTED.txt\n",
    "!head -n 100 wordcount-output-sorted/part-00000.SORTED.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identityMapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count for Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 15:43:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted historical_tours.txt\n",
      "Found 8 items\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-03 02:41 Enron_email_reading_output\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-03 02:19 Sort\n",
      "-rw-r--r--   1 hadoop hadoop     204678 2016-06-03 02:41 enronemail_1h.txt\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-04 15:42 gutenberg-wordCount\n",
      "-rw-r--r--   1 hadoop hadoop       1318 2016-06-04 15:43 historical_tours.txt\n",
      "-rw-r--r--   1 hadoop hadoop         56 2016-06-04 15:41 testWordCountInput.txt\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-04 15:39 wordcount-output\n",
      "drwxr-xr-x   - hadoop hadoop          0 2016-06-01 02:07 wordcount-output-sorted\n",
      "16/06/04 15:43:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted testWordCountInput.txt\n",
      "16/06/04 15:43:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-wordCount\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar] /tmp/streamjob8591669224831412084.jar tmpDir=null\n",
      "16/06/04 15:43:36 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/04 15:43:36 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-7-251.us-west-1.compute.internal/172.31.7.251:8032\n",
      "16/06/04 15:43:36 INFO metrics.MetricsSaver: MetricsConfigRecord disabledInCluster: false instanceEngineCycleSec: 60 clusterEngineCycleSec: 60 disableClusterEngine: true maxMemoryMb: 3072 maxInstanceCount: 500 lastModified: 1464726748890 \n",
      "16/06/04 15:43:36 INFO metrics.MetricsSaver: Created MetricsSaver j-ZAC3GQDMC0E6:i-610a91d4:RunJar:14769 period:60 /mnt/var/em/raw/i-610a91d4_20160604_RunJar_14769_raw.bin\n",
      "16/06/04 15:43:37 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "16/06/04 15:43:37 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 426d94a07125cf9447bb0c2b336cf10b4c254375]\n",
      "16/06/04 15:43:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 15:43:37 INFO mapreduce.JobSubmitter: number of splits:19\n",
      "16/06/04 15:43:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464726740139_0025\n",
      "16/06/04 15:43:37 INFO impl.YarnClientImpl: Submitted application application_1464726740139_0025\n",
      "16/06/04 15:43:37 INFO mapreduce.Job: The url to track the job: http://ip-172-31-7-251.us-west-1.compute.internal:20888/proxy/application_1464726740139_0025/\n",
      "16/06/04 15:43:37 INFO mapreduce.Job: Running job: job_1464726740139_0025\n",
      "16/06/04 15:43:45 INFO mapreduce.Job: Job job_1464726740139_0025 running in uber mode : false\n",
      "16/06/04 15:43:45 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 15:43:53 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000008_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:43:56 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000009_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:43:59 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000010_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:03 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000008_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:03 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000002_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:03 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000004_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:04 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000005_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:04 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000006_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:04 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000003_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:04 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:04 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:04 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000007_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:06 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "16/06/04 15:44:06 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000011_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 15:44:09 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000009_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:10 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000012_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:11 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000013_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:12 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000014_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:13 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000010_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:16 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000008_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:17 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000002_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:21 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000006_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:21 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000004_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:23 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000005_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:24 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000011_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:24 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000003_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:25 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000009_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:26 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000012_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:26 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:27 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000014_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:27 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:29 INFO mapreduce.Job: Task Id : attempt_1464726740139_0025_m_000007_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:172)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 9 more\n",
      "Caused by: java.lang.RuntimeException: Error in configuring object\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:42)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n",
      "\t... 17 more\n",
      "Caused by: java.lang.RuntimeException: configuration exception\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:232)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66)\n",
      "\t... 22 more\n",
      "Caused by: java.io.IOException: Cannot run program \"WordCount/mapper.py\": error=2, No such file or directory\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:219)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:187)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:130)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)\n",
      "\t... 24 more\n",
      "\n",
      "16/06/04 15:44:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 15:44:32 INFO mapreduce.Job: Job job_1464726740139_0025 failed with state FAILED due to: Task failed task_1464726740139_0025_m_000008\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "16/06/04 15:44:32 INFO mapreduce.Job: Counters: 18\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=32\n",
      "\t\tKilled map tasks=18\n",
      "\t\tKilled reduce tasks=3\n",
      "\t\tLaunched map tasks=44\n",
      "\t\tOther local map tasks=29\n",
      "\t\tData-local map tasks=9\n",
      "\t\tRack-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22583340\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=501852\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=501852\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=722666880\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "16/06/04 15:44:32 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm historical_tours.txt \n",
    "!hdfs dfs -copyFromLocal WordCount/historical_tours.txt \n",
    "!hdfs dfs -ls\n",
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r gutenberg-wordCount\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -combiner WordCount/reducer.py \\\n",
    "   -input testWordCountInput.txt \\\n",
    "   -output gutenberg-wordCount  \\\n",
    "   -numReduceTasks 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:50:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:50:52 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-output-sorted\n",
      "16/02/02 10:50:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:50:54 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:50:54 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:50:54 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/02/02 10:50:54 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/02/02 10:50:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2068509560_0001\n",
      "16/02/02 10:50:54 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:50:54 INFO mapreduce.Job: Running job: job_local2068509560_0001\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000000_0\n",
      "16/02/02 10:50:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00000:0+9528\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:50:54 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: Records R/W=986/1\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufend = 9528; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210456(104841824); length = 3941/6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Records R/W=986/1\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000000_0' done.\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000000_0\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000001_0\n",
      "16/02/02 10:50:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00003:0+9073\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: Records R/W=931/1\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: bufstart = 0; bufend = 9073; bufvoid = 104857600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210676(104842704); length = 3721/6553600\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Records R/W=931/1\n",
      "16/02/02 10:50:54 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000001_0' done.\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000001_0\n",
      "16/02/02 10:50:54 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000002_0\n",
      "16/02/02 10:50:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00002:0+8977\n",
      "16/02/02 10:50:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: Records R/W=921/1\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufend = 8977; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210716(104842864); length = 3681/6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Records R/W=921/1\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000002_0' done.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000002_0\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_m_000003_0\n",
      "16/02/02 10:50:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00001:0+8970\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./identityMapper.py]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: Records R/W=898/1\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:50:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: bufstart = 0; bufend = 8970; bufvoid = 104857600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210808(104843232); length = 3589/6553600\n",
      "16/02/02 10:50:55 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task:attempt_local2068509560_0001_m_000003_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Records R/W=898/1\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task 'attempt_local2068509560_0001_m_000003_0' done.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_m_000003_0\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2068509560_0001_r_000000_0\n",
      "16/02/02 10:50:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:50:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:50:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5de1209c\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:50:55 INFO reduce.EventFetcher: attempt_local2068509560_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000002_0 decomp: 10821 len: 10825 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 10821 bytes from map-output for attempt_local2068509560_0001_m_000002_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10821, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10821\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000001_0 decomp: 10937 len: 10941 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 10937 bytes from map-output for attempt_local2068509560_0001_m_000001_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10937, inMemoryMapOutputs.size() -> 2, commitMemory -> 10821, usedMemory ->21758\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000000_0 decomp: 11502 len: 11506 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 11502 bytes from map-output for attempt_local2068509560_0001_m_000000_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11502, inMemoryMapOutputs.size() -> 3, commitMemory -> 21758, usedMemory ->33260\n",
      "16/02/02 10:50:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068509560_0001_m_000003_0 decomp: 10768 len: 10772 to MEMORY\n",
      "16/02/02 10:50:55 INFO reduce.InMemoryMapOutput: Read 10768 bytes from map-output for attempt_local2068509560_0001_m_000003_0\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10768, inMemoryMapOutputs.size() -> 4, commitMemory -> 33260, usedMemory ->44028\n",
      "16/02/02 10:50:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 43985 bytes\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: Merged 4 segments, 44028 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: Merging 1 files, 44026 bytes from disk\n",
      "16/02/02 10:50:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:50:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 44012 bytes\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task:attempt_local2068509560_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task attempt_local2068509560_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:50:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2068509560_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output-sorted/_temporary/0/task_local2068509560_0001_r_000000\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/02 10:50:55 INFO mapred.Task: Task 'attempt_local2068509560_0001_r_000000_0' done.\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068509560_0001_r_000000_0\n",
      "16/02/02 10:50:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:50:55 INFO mapreduce.Job: Job job_local2068509560_0001 running in uber mode : false\n",
      "16/02/02 10:50:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:50:55 INFO mapreduce.Job: Job job_local2068509560_0001 completed successfully\n",
      "16/02/02 10:50:55 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=620270\n",
      "\t\tFILE: Number of bytes written=2079832\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=128803\n",
      "\t\tHDFS: Number of bytes written=36548\n",
      "\t\tHDFS: Number of read operations=51\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=7\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=3736\n",
      "\t\tMap output bytes=36548\n",
      "\t\tMap output materialized bytes=44044\n",
      "\t\tInput split bytes=464\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3736\n",
      "\t\tReduce shuffle bytes=44044\n",
      "\t\tReduce input records=3736\n",
      "\t\tReduce output records=3736\n",
      "\t\tSpilled Records=7472\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=2378694656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=36548\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36548\n",
      "16/02/02 10:50:55 INFO streaming.StreamJob: Output directory: gutenberg-output-sorted\n"
     ]
    }
   ],
   "source": [
    "#Sort the words in descreasing order of frequency, but break ties with alphabetical sorting\n",
    "!hdfs dfs -rm -r gutenberg-output-sorted\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -mapper identityMapper.py \\\n",
    "   -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "   -input gutenberg-output \\\n",
    "   -output gutenberg-output-sorted  \\\n",
    "   -numReduceTasks 1 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:51:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup      36548 2016-02-02 10:50 gutenberg-output-sorted/part-00000\n",
      "16/02/02 10:51:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "States,\t6\n",
      "Starting\t3\n",
      "Start:\t1\n",
      "Start\t1\n",
      "Students\t1\n",
      "Station,\t2\n",
      "Standish\t2\n",
      "Square;\t1\n",
      "Swing\t2\n",
      "Sunday.\t1\n",
      "Sumner\t2\n",
      "Sullivan\t1\n",
      "Standish's\t1\n",
      "Tavern\"\t1\n",
      "Shows\t1\n",
      "Sheraton\t1\n",
      "Sewall\"\t1\n",
      "Sewall\t1\n",
      "Simmons\t2\n",
      "Sidney\t2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls \"gutenberg-output-sorted/part-*\"\n",
    "!rm -r gutenberg-output-sorted\n",
    "!hdfs dfs -copyToLocal \"gutenberg-output-sorted\" \n",
    "!head -n 20 gutenberg-output-sorted/part-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States,\t6\r\n",
      "Starting\t3\r\n",
      "Start:\t1\r\n",
      "Start\t1\r\n",
      "Students\t1\r\n",
      "Station,\t2\r\n",
      "Standish\t2\r\n",
      "Square;\t1\r\n",
      "Swing\t2\r\n",
      "Sunday.\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 gutenberg-output-sorted/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 08:40:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "windows.\t1\n",
      "winter.\t1\n",
      "wish\t2\n",
      "within\t6\n",
      "words\t1\n",
      "work.\t5\n",
      "would\t2\n",
      "wrote\t3\n",
      "your\t52\n",
      "yourself\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail  gutenberg-output/part-00000 |tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Address secondary sort and paritioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddresses.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddresses.txt\n",
    "11.12.1.2\n",
    "11.14.2.3\n",
    "11.11.4.1\n",
    "11.11.3.1\n",
    "11.11.6.1\n",
    "11.11.7.1\n",
    "11.12.1.1\n",
    "11.14.2.2\n",
    "11.12.2.5222\n",
    "11.9999999.2.5222\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:06:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/02 10:06:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/02 10:06:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:06:59 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:06:59 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:06:59 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:06:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 10:07:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 10:07:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local378757331_0001\n",
      "16/02/02 10:07:00 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:07:00 INFO mapreduce.Job: Running job: job_local378757331_0001\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Starting task: attempt_local378757331_0001_m_000000_0\n",
      "16/02/02 10:07:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:07:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: bufstart = 0; bufend = 88; bufvoid = 104857600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/02 10:07:00 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task:attempt_local378757331_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task 'attempt_local378757331_0001_m_000000_0' done.\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local378757331_0001_m_000000_0\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Starting task: attempt_local378757331_0001_r_000000_0\n",
      "16/02/02 10:07:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:07:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:07:00 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3eaf4e6f\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:07:00 INFO reduce.EventFetcher: attempt_local378757331_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:07:00 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local378757331_0001_m_000000_0 decomp: 104 len: 108 to MEMORY\n",
      "16/02/02 10:07:00 INFO reduce.InMemoryMapOutput: Read 104 bytes from map-output for attempt_local378757331_0001_m_000000_0\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 104, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->104\n",
      "16/02/02 10:07:00 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: Merged 1 segments, 104 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: Merging 1 files, 108 bytes from disk\n",
      "16/02/02 10:07:00 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:07:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:07:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 10:07:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task:attempt_local378757331_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task attempt_local378757331_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:07:00 INFO output.FileOutputCommitter: Saved output of task 'attempt_local378757331_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local378757331_0001_r_000000\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/02 10:07:00 INFO mapred.Task: Task 'attempt_local378757331_0001_r_000000_0' done.\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local378757331_0001_r_000000_0\n",
      "16/02/02 10:07:00 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:07:01 INFO mapreduce.Job: Job job_local378757331_0001 running in uber mode : false\n",
      "16/02/02 10:07:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:07:01 INFO mapreduce.Job: Job job_local378757331_0001 completed successfully\n",
      "16/02/02 10:07:01 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210518\n",
      "\t\tFILE: Number of bytes written=752678\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=160\n",
      "\t\tHDFS: Number of bytes written=88\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=88\n",
      "\t\tMap output materialized bytes=108\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=108\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=727711744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88\n",
      "16/02/02 10:07:01 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom partitioner  and sort works locally and on AWS EM Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:23:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/02 10:23:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/02 10:23:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 10:23:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/02 10:23:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/02 10:23:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/02 10:23:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/02 10:23:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/02 10:23:48 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/02 10:23:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1055179505_0001\n",
      "16/02/02 10:23:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/02 10:23:49 INFO mapreduce.Job: Running job: job_local1055179505_0001\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+110\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: \n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Spilling map output\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: bufstart = 0; bufend = 121; bufvoid = 104857600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
      "16/02/02 10:23:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_m_000000_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_r_000000_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b654ebd\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: attempt_local1055179505_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:23:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1055179505_0001_m_000000_0 decomp: 44 len: 48 to MEMORY\n",
      "16/02/02 10:23:49 INFO reduce.InMemoryMapOutput: Read 44 bytes from map-output for attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 44, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->44\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 44 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 1 files, 48 bytes from disk\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task attempt_local1055179505_0001_r_000000_0 is allowed to commit now\n",
      "16/02/02 10:23:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055179505_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1055179505_0001_r_000000\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_r_000000_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_r_000000_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_r_000001_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5fa99f18\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: attempt_local1055179505_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:23:49 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1055179505_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/02 10:23:49 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task attempt_local1055179505_0001_r_000001_0 is allowed to commit now\n",
      "16/02/02 10:23:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055179505_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1055179505_0001_r_000001\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_r_000001_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_r_000001_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1055179505_0001_r_000002_0\n",
      "16/02/02 10:23:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/02 10:23:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/02 10:23:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1dc18346\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: attempt_local1055179505_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/02 10:23:49 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1055179505_0001_m_000000_0 decomp: 101 len: 105 to MEMORY\n",
      "16/02/02 10:23:49 INFO reduce.InMemoryMapOutput: Read 101 bytes from map-output for attempt_local1055179505_0001_m_000000_0\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 101, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->101\n",
      "16/02/02 10:23:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89 bytes\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 101 bytes to disk to satisfy reduce memory limit\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 1 files, 105 bytes from disk\n",
      "16/02/02 10:23:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/02 10:23:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89 bytes\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/02 10:23:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task:attempt_local1055179505_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task attempt_local1055179505_0001_r_000002_0 is allowed to commit now\n",
      "16/02/02 10:23:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1055179505_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1055179505_0001_r_000002\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/02 10:23:49 INFO mapred.Task: Task 'attempt_local1055179505_0001_r_000002_0' done.\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1055179505_0001_r_000002_0\n",
      "16/02/02 10:23:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/02 10:23:50 INFO mapreduce.Job: Job job_local1055179505_0001 running in uber mode : false\n",
      "16/02/02 10:23:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/02 10:23:50 INFO mapreduce.Job: Job job_local1055179505_0001 completed successfully\n",
      "16/02/02 10:23:50 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422085\n",
      "\t\tFILE: Number of bytes written=1515073\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=440\n",
      "\t\tHDFS: Number of bytes written=193\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=121\n",
      "\t\tMap output materialized bytes=159\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=159\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1453326336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=121\n",
      "16/02/02 10:23:50 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "#Custom partitioner works \n",
    "# partition based on first parts\n",
    "#sort numerically decreasing on the third part\n",
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D map.output.key.field.separator=. \\\n",
    "  -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.job.reduces=3 \\\n",
    "  -input ipAddresses.txt \\\n",
    "  -output myOutputDirForIPAddresses \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 10:23:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.12.2.5222\t\n",
      "11.12.1.1\t\n",
      "11.12.1.2\t\n",
      "==========================\n",
      "16/02/02 10:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "==========================\n",
      "16/02/02 10:23:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.11.7.1\t\n",
      "11.11.6.1\t\n",
      "11.11.4.1\t\n",
      "11.11.3.1\t\n",
      "11.9999999.2.5222\t\n",
      "11.14.2.2\t\n",
      "11.14.2.3\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00000\n",
    "!echo \"==========================\"\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00001\n",
    "!echo \"==========================\"\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a job on AWS by launch an EMR Cluster\n",
    "** custom partition and sort works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls /usr/lib/hadoop/hadoop-streaming-2.7.2-amzn-1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run a job on AWS by launch an EMR Cluster\n",
    "#remote login to cluster\n",
    "\n",
    "#on AWS generate sample data file called ipAddresses.txt\n",
    "echo -e \"11.12.1.2\" > ipAddresses.txt\n",
    "echo -e \"11.14.2.3\" >> ipAddresses.txt\n",
    "echo -e \"11.11.4.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.7.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.6.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.5.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.4.1\" >> ipAddresses.txt\n",
    "echo -e \"11.11.9.1\" >> ipAddresses.txt\n",
    "echo -e \"11.12.9.1\" >> ipAddresses.txt\n",
    "echo -e \"11.14.2.2\" >> ipAddresses.txt\n",
    "echo -e \"11.12.2.5222\" >> ipAddresses.txt\n",
    "echo -e \"11.9999999.2.5222\" >> ipAddresses.txt\n",
    "\n",
    "\n",
    "#NOTE Hadoop is already running!\n",
    "#\n",
    "hdfs dfs -rm ipAddresses.txt \n",
    "hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "hdfs dfs -ls\n",
    "#no need to start hadoop on an EMR cluster. It is starts at cluster launch time\n",
    "# check by typing hdfs dfs -l\n",
    "hdfs dfs -rm -r myOutputDirForIPAddresses\n",
    "\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.1-amzn-0.jar \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D map.output.key.field.separator=. \\\n",
    "  -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.job.reduces=3 \\\n",
    "  -input ipAddresses.txt \\\n",
    "  -output myOutputDirForIPAddresses \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "\n",
    "hdfs dfs -cat myOutputDirForIPAddresses/*\n",
    "    # This job failed as it did partition the records correctly (i.e., by column 1 and 2)\n",
    "# as required by \n",
    "#         mapred.text.key.partitioner.options=-k1,2 \n",
    "[hadoop@ip-172-31-15-64 ~]$ hdfs dfs -cat myOutputDirForIPAddresses/*\n",
    "11.12.9.1\t\n",
    "11.12.2.5222\t\n",
    "11.12.1.2\t\n",
    "11.11.9.1\t\n",
    "11.11.7.1\t\n",
    "11.11.6.1\t\n",
    "11.11.5.1\t\n",
    "11.11.4.1\t\n",
    "11.11.4.1\t\n",
    "11.14.2.3\t\n",
    "11.14.2.2\t\n",
    "11.9999999.2.5222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-241-02d4bae2e0e3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-241-02d4bae2e0e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scp ipAddresses.txt\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scp ipAddresses.txt\n",
    "!sort -d\".\" -k1,1 -k2,2 <ipAddresses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.12.1.2\r\n",
      "11.14.2.3\r\n",
      "11.11.4.1\r\n",
      "11.12.1.1\r\n",
      "11.14.2.2\r\n",
      "11.12.2.5222\r\n",
      "11.9999999.2.5222"
     ]
    }
   ],
   "source": [
    "!cat ipAddresses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 08:47:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 08:47:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/02 08:47:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/02 08:47:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 15 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 16:57 gutenberg-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 08:45 gutenberg-output-sorted\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 08:34 gutenberg-wordCount\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2016-02-02 08:34 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         80 2016-02-02 08:47 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:36 output-secondarysort-streaming\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:46 stockprice\n",
      "-rw-r--r--   1 jshanahan supergroup        400 2016-02-01 12:46 stockprice.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         56 2016-02-02 08:34 testWordCountInput.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 18:09 wordcount-output\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-02 08:45 wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddressMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddressMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    f1, f2, f3, f4 = line.split('.')\n",
    "    # get date from unix time\n",
    "    print '%s\\t%s\\t%s\\t%s mapper' % (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddressReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddressReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    f1, f2, f3, f4 = line.split('\\t')\n",
    "    # get date from unix time\n",
    "    print '%s.%s.%s.%s jimi' % (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.12.1.2 jimi\r\n",
      "11.14.2.3 jimi\r\n",
      "11.11.4.1 jimi\r\n",
      "11.12.1.1 jimi\r\n",
      "11.14.2.2 jimi\r\n",
      "11.12.2.5222 jimi\r\n",
      "11.9999999.2.5222 jimi\r\n"
     ]
    }
   ],
   "source": [
    "!cat ipAddresses.txt| python ipAddressReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count with a Custom partitioner step \n",
    "**To enable total sorting of the word count frequency when multiple reducers are used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapperWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapperWithPartitionTable.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are you\\n\")\n",
    "\n",
    "group1 = \"abcdefghijklm\"\n",
    "group2 = \"nopqrstuvwxyz\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        firstChar = word[0].lower()\n",
    "        if firstChar in group1:\n",
    "            print \"group1\\t%s\\t%d\" %(word, 1)\n",
    "        elif firstChar in group2:\n",
    "            print \"group2\\t%s\\t%d\" %(word, 1)\n",
    "        else:\n",
    "            print \"group3\\t%s\\t%d\" %(word, 1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "group1\tbar\t1\r\n",
      "group1\tfoo\t1\r\n",
      "group1\tfoo\t1\r\n",
      "group1\tfoo\t1\r\n",
      "group1\tlabs\t1\r\n",
      "group2\tquux\t1\r\n",
      "group2\tquux\t1\r\n",
      "group2\tst#ff\t1\r\n",
      "group3\t#funky\t1\r\n"
     ]
    }
   ],
   "source": [
    "#partition the word count records\n",
    "!echo \"foo foo quux labs foo #funky st#ff bar quux\" | python WordCount/mapperWithPartitionTable.py | sort -k1,1 -k2,2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducerWithPartitionKey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducerWithPartitionKey.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    group, key, value = line.split()   #one minor modification to process the parition key. I.e., drop it\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "foo\t3\r\n",
      "quux\t2\r\n",
      "#funky\t1\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n",
      "st#ff\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo #funky st#ff bar quux\" | python WordCount/mapperWithPartitionTable.py | \\\n",
    " sort -k1,1 -k2,2 | \\\n",
    " python WordCount/reducerWithPartitionKey.py |\\\n",
    " sort -k2,2nr -k1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:53:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:53:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:53:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:53:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:53:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:53:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:53:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1733440059_0001\n",
      "16/02/01 12:53:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:53:49 INFO mapreduce.Job: Running job: job_local1733440059_0001\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: bufstart = 0; bufend = 137; bufvoid = 104857600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_m_000000_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000000_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4dcd971d\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 70 len: 74 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 70 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 70, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->70\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 60 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 70 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 74 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 60 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000000\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000000_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000000_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000001_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d946c52\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000001\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000001_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000001_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000002_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@38a63406\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 45 len: 49 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 45 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 45, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 35 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 45 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 49 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 35 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000002\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000002_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000002_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Job job_local1733440059_0001 running in uber mode : false\n",
      "16/02/01 12:53:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Job job_local1733440059_0001 completed successfully\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422129\n",
      "\t\tFILE: Number of bytes written=1498547\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=320\n",
      "\t\tHDFS: Number of bytes written=219\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=137\n",
      "\t\tMap output materialized bytes=169\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=169\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1440743424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=101\n",
      "16/02/01 12:53:50 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "#Partition into 3 reducers (the first 2 fields are used as keys for partition)\n",
    "#Sorting within each partition for the reducer(all 4 fields used for sorting)\n",
    "#In the above example, \"-D stream.map.output.field.separator=.\" specifies \".\" as the field separator \n",
    "#for the map outputs, and the prefix up to the fourth \".\" in a line will be the key and the rest of \n",
    "#the line (excluding the fourth \".\") will be the value. If a line has less than four \".\"s, then the \n",
    "#whole line will be the key and the value will be an empty Text object (like the one created by new Text(\"\")).\n",
    "\n",
    "#Similarly, you can use \"-D stream.reduce.output.field.separator=SEP\" and \"-D stream.num.reduce.output.fields=NUM\"\n",
    "#to specify the nth field separator in a line of the reduce outputs as the separator between the key and the value.\n",
    "\n",
    "#Similarly, you can specify \"stream.map.input.field.separator\" and \"stream.reduce.input.field.separator\" as \n",
    "#the input separator for Map/Reduce inputs. By default the separator is the tab character.\n",
    "\n",
    "#record 11.12.1.2 is treated as a key only with no value\n",
    "\n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:56:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:56:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:57:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:57:00 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:57:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:57:00 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:57:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:57:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/02/01 12:57:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1377900324_0001\n",
      "16/02/01 12:57:01 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:57:01 INFO mapreduce.Job: Running job: job_local1377900324_0001\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: numReduceTasks: 5\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressMapper.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: bufstart = 0; bufend = 137; bufvoid = 104857600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_m_000000_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000000_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@450849b9\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 22 len: 26 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 22 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 22, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->22\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 22 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 26 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000000\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000000_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000000_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000001_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c2c6e85\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 65 len: 69 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 65 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 65, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->65\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 65 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 69 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000001\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000001_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000001_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000002_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@449a3b50\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 30 len: 34 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 30 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 30, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->30\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 30 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 34 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000002\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000002_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000002_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000003_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6665f62e\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 23 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 23 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000003_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000003\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000003_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000003_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000004_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3b5a3cf4\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000004_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#5 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000004_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000004_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000004_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000004\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000004_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000004_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Job job_local1377900324_0001 running in uber mode : false\n",
      "16/02/01 12:57:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Job job_local1377900324_0001 completed successfully\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=635229\n",
      "\t\tFILE: Number of bytes written=2265774\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=480\n",
      "\t\tHDFS: Number of bytes written=588\n",
      "\t\tHDFS: Number of read operations=75\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=137\n",
      "\t\tMap output materialized bytes=181\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=181\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=2205679616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=172\n",
      "16/02/01 12:57:02 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapred.text.key.partitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper ipAddressMapper.py \\\n",
    "    -reducer ipAddressReducer.py \\\n",
    "    -numReduceTasks 5 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sorting within each partition for the reducer(all 4 fields used for sorting)\n",
    "# desired output \n",
    "11.11.4.1\n",
    "-----------\n",
    "11.12.1.1\n",
    "11.12.1.2\n",
    "-----------\n",
    "11.14.2.2\n",
    "11.14.2.3\n",
    "11.14.2.5222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:57:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 6 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         23 2016-02-01 12:57 myOutputDirForIPAddresses/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup         72 2016-02-01 12:57 myOutputDirForIPAddresses/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         31 2016-02-01 12:57 myOutputDirForIPAddresses/part-00002\n",
      "-rw-r--r--   1 jshanahan supergroup         46 2016-02-01 12:57 myOutputDirForIPAddresses/part-00003\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses/part-00004\n",
      "16/02/01 12:57:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.11.4.1 mapper jimi\t\n",
      "----\n",
      "16/02/01 12:57:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.12.1.1 mapper jimi\t\n",
      "11.12.1.2 mapper jimi\t\n",
      "11.12.2.5222 mapper jimi\t\n",
      "----\n",
      "16/02/01 12:57:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.9999999.2.5222 mapper jimi\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls myOutputDirForIPAddresses\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00000\n",
    "!echo \"----\" \n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00001\n",
    "!echo \"----\" \n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 10:08:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 10:08:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 10:08:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 10:08:43 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 10:08:43 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 10:08:43 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 10:08:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 10:08:43 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 10:08:43 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/01 10:08:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local145025840_0001\n",
      "16/02/01 10:08:44 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 10:08:44 INFO mapreduce.Job: Running job: job_local145025840_0001\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+59\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: \n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: bufstart = 0; bufend = 108; bufvoid = 104857600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+59\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_m_000000_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000000_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5748fba6\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000000\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000000_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000000_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000001_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@576ca50f\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000001\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000001_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000001_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000002_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50bda288\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000002\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000002_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000002_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Job job_local145025840_0001 running in uber mode : false\n",
      "16/02/01 10:08:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Job job_local145025840_0001 completed successfully\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=421940\n",
      "\t\tFILE: Number of bytes written=1498172\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=236\n",
      "\t\tHDFS: Number of bytes written=153\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=108\n",
      "\t\tMap output materialized bytes=138\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=138\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1333788672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=77\n",
      "16/02/01 10:08:45 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D map.output.key.field.separator=. \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:55:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:55:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:55:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:55:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:55:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:55:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:55:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:55:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:55:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local72918067_0001\n",
      "16/02/01 12:55:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:55:35 INFO mapreduce.Job: Running job: job_local72918067_0001\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: Starting task: attempt_local72918067_0001_m_000000_0\n",
      "16/02/01 12:55:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:55:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:55:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:55:36 WARN mapred.LocalJobRunner: job_local72918067_0001\n",
      "java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\n",
      "Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable\n",
      "\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1069)\n",
      "\tat org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:607)\n",
      "\tat org.apache.hadoop.mapred.lib.IdentityMapper.map(IdentityMapper.java:43)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Job job_local72918067_0001 running in uber mode : false\n",
      "16/02/01 12:55:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Job job_local72918067_0001 failed with state FAILED due to: NA\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Counters: 0\n",
      "16/02/01 12:55:36 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "     -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer ipAddressReducer.py \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n",
    "   -D map.output.key.field.separator=. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 10:09:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0\t11.12.1.2\n",
      "30\t11.12.1.1\n",
      "10\t11.14.2.3\n",
      "40\t11.14.2.2\n",
      "20\t11.11.4.1\n",
      "50\t11.14.2.5\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Sort: Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockprice.txt\n",
    "Google,2015-08-06,647.32\n",
    "Apple,2015-08-01,117.83\n",
    "Facebook,2015-08-05,92.67\n",
    "Facebook,2015-08-01,90.96\n",
    "Oracle,2015-08-04,38.55\n",
    "Apple,2015-08-04,113.77\n",
    "Google,2015-08-05,677.95\n",
    "Facebook,2015-08-08,90.43\n",
    "Oracle,2015-08-03,35.78\n",
    "Apple,2015-08-11,110.09\n",
    "Oracle,2015-08-07,39.67\n",
    "Google,2015-08-09,656.63\n",
    "ABXXXX,2015-08-07,39.67\n",
    "ABXXXX,2015-08-09,656.63\n",
    "Google,2000-08-09,0\n",
    "ABXXXX,2015-08-08,6569999999999.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockPriceMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockPriceMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line\n",
    "    name, date, price= line.split(\",\")\n",
    "    # unix time is for secondary sort\n",
    "    unix_time = datetime.datetime.strptime(date, '%Y-%m-%d').strftime(\"%s\")\n",
    "    # output each record\n",
    "    print '%s\\t%s\\t%s' % (name, unix_time, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockPriceReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockPriceReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    name, unix_time, price = line.split('\\t')\n",
    "    # get date from unix time\n",
    "    date = datetime.datetime.fromtimestamp(int(unix_time)).strftime('%Y-%m-%d')\n",
    "    print '%s\\t%s\\t%s' % (name, date, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Google\t965804400\t0\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#sort stock name increasing, and the do a secondary sort on key2=unix date, NUMERICALLY\n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2n   #Unix sort\n",
    "#WHY does it NOT work? It takes the whole as a key and sorts alphanumerically\n",
    "\n",
    "#very different to the following line -k1,1 sort \n",
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t965804400\t0\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#sort stock name increasing, and the do a secondary sort on key2=unix date, NUMERICALLY\n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1,1 -k2,2n   #Unix sort\n",
    "#this is CORRECT\n",
    "#very different to the following line -k1,1 sort \n",
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Google\t965804400\t0\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1,1 -k2,2n   #Unix sort\n",
    "#very different to the following line -k1,1 sort \n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice.txt\n",
      "16/02/01 12:46:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm stockprice.txt\n",
    "!hdfs dfs -copyFromLocal stockprice.txt /user/jshanahan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice\n",
      "16/02/01 12:46:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:42 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:46:42 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:46:42 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:46:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:46:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:46:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local249202583_0001\n",
      "16/02/01 12:46:43 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:46:43 INFO mapreduce.Job: Running job: job_local249202583_0001\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/stockprice.txt:0+400\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceMapper.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=16/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: bufstart = 0; bufend = 416; bufvoid = 104857600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214336(104857344); length = 61/6553600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=16/1\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_m_000000_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_r_000000_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ebfefd\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: attempt_local249202583_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:46:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local249202583_0001_m_000000_0 decomp: 251 len: 255 to MEMORY\n",
      "16/02/01 12:46:43 INFO reduce.InMemoryMapOutput: Read 251 bytes from map-output for attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 251, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->251\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 225 bytes\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 251 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 255 bytes from disk\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 225 bytes\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceReducer.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=9/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task attempt_local249202583_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:46:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local249202583_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/stockprice/_temporary/0/task_local249202583_0001_r_000000\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_r_000000_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_r_000000_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_r_000001_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b31deb3\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: attempt_local249202583_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:46:43 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local249202583_0001_m_000000_0 decomp: 201 len: 205 to MEMORY\n",
      "16/02/01 12:46:43 INFO reduce.InMemoryMapOutput: Read 201 bytes from map-output for attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 201, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->201\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 174 bytes\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 201 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 205 bytes from disk\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 174 bytes\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceReducer.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task attempt_local249202583_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:46:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local249202583_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/stockprice/_temporary/0/task_local249202583_0001_r_000001\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_r_000001_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_r_000001_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Job job_local249202583_0001 running in uber mode : false\n",
      "16/02/01 12:46:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Job job_local249202583_0001 completed successfully\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=317416\n",
      "\t\tFILE: Number of bytes written=1132117\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1200\n",
      "\t\tHDFS: Number of bytes written=623\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=16\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=416\n",
      "\t\tMap output materialized bytes=460\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16\n",
      "\t\tReduce shuffle bytes=460\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=16\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=781713408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=400\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=401\n",
      "16/02/01 12:46:44 INFO streaming.StreamJob: Output directory: stockprice\n"
     ]
    }
   ],
   "source": [
    "#NOTE \"-k1,1 -k2,2nr\" -k1,1  is redundanct\n",
    "!hdfs dfs -rm -r stockprice\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "        -D stream.num.map.output.key.fields=3 \\\n",
    "        -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "        -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "        -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "        -mapper stockPriceMapper.py \\\n",
    "        -reducer stockPriceReducer.py \\\n",
    "        -input stockprice.txt -output stockprice \\\n",
    "        -numReduceTasks 2 \\\n",
    "        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:46 stockprice/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup        222 2016-02-01 12:46 stockprice/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        179 2016-02-01 12:46 stockprice/part-00001\n",
      "16/02/01 12:46:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Apple\t2015-08-11\t110.09\n",
      "Apple\t2015-08-04\t113.77\n",
      "Apple\t2015-08-01\t117.83\n",
      "Facebook\t2015-08-08\t90.43\n",
      "Facebook\t2015-08-05\t92.67\n",
      "Facebook\t2015-08-01\t90.96\n",
      "Oracle\t2015-08-07\t39.67\n",
      "Oracle\t2015-08-04\t38.55\n",
      "Oracle\t2015-08-03\t35.78\n",
      "ABXXXX\t2015-08-09\t656.63\n",
      "ABXXXX\t2015-08-08\t6569999999999.63\n",
      "ABXXXX\t2015-08-07\t39.67\n",
      "Google\t2015-08-09\t656.63\n",
      "Google\t2015-08-06\t647.32\n",
      "Google\t2015-08-05\t677.95\n",
      "Google\t2000-08-09\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls stockprice\n",
    "!hdfs dfs -cat stockprice/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:39:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ABXXXX\t2015-08-09\t656.63\n",
      "ABXXXX\t2015-08-07\t39.67\n",
      "Google\t2015-08-09\t656.63\n",
      "Google\t2015-08-06\t647.32\n",
      "Google\t2015-08-05\t677.95\n",
      "Google\t2000-08-09\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat stockprice/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r stockprice\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -files secondary_sort_map.py,secondary_sort_reduce.py \\\n",
    "    -input SecondarySort \\\n",
    "    -output output-secondarysort-streaming \\\n",
    "    -mapper secondary_sort_map.py \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -reducer secondary_sort_reduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:39:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-01-27 22:39 stockprice/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         75 2016-01-27 22:39 stockprice/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        222 2016-01-27 22:39 stockprice/part-00001\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls stockprice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:40:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Apple\t2015-08-11\t110.09\n",
      "Apple\t2015-08-04\t113.77\n",
      "Apple\t2015-08-01\t117.83\n",
      "Facebook\t2015-08-08\t90.43\n",
      "Facebook\t2015-08-01\t90.96\n",
      "Facebook\t2015-08-05\t92.67\n",
      "Oracle\t2015-08-07\t39.67\n",
      "Oracle\t2015-08-03\t35.78\n",
      "Oracle\t2015-08-04\t38.55\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat stockprice/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Sort from Tom White (Chapter 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SecondarySort/otherYears.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile SecondarySort/otherYears.txt\n",
    "0029029070999991903010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991904010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "0029029070999991907010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "002902907099999190610106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991906010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "0029029070999991906010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991907010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991907010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:23:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "copyFromLocal: `SecondarySort/1901': File exists\n",
      "copyFromLocal: `SecondarySort/1902': File exists\n",
      "copyFromLocal: `SecondarySort/otherYears.txt': File exists\n",
      "16/02/01 09:23:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 9 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-30 09:23 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         59 2016-01-31 20:01 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-31 20:36 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 output-secondarysort-streaming\n",
      "-rw-r--r--   1 jshanahan supergroup        365 2016-02-01 08:55 stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "#copy year 1901 and 1902 to HDFS\n",
    "#val[15:19], int(val[87:92]), val[92:93]\n",
    "# year, temp, q\n",
    "#002902907099999  1901  010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "#0029029070999991901010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "#0029029070999991901010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "\n",
    "\n",
    "!hdfs dfs -copyFromLocal SecondarySort\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondary_sort_map.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondary_sort_map.py\n",
    "#!/usr/bin/env python\n",
    "#Example 9-7. Map function for secondary sort in Python\n",
    "import re\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    val = line.strip()\n",
    "    (year, temp, q) = (val[15:19], int(val[87:92]), val[92:93])\n",
    "    if temp == 9999:\n",
    "        sys.stderr.write(\"reporter:counter:Temperature,Missing,1\\n\")\n",
    "    elif re.match(\"[01459]\", q):\n",
    "        print \"%s\\t%s\" % (year, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondary_sort_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondary_sort_reduce.py\n",
    "#!/usr/bin/env python\n",
    "#Example 9-8. Reducer function for secondary sort in Python\n",
    "import sys\n",
    "last_group = None\n",
    "for line in sys.stdin:\n",
    "    val = line.strip()\n",
    "    (year, temp) = val.split(\"\\t\")\n",
    "    group = year\n",
    "    if last_group != group:   #print the first record ONLY for each year; skip all other records for that year\n",
    "        print val\n",
    "        last_group = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902\t-100\r\n"
     ]
    }
   ],
   "source": [
    "!cat SecondarySort/1902| python secondary_sort_map.py |sort|python secondary_sort_reduce.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:36:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 09:36:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted output-secondarysort-streaming\n",
      "16/02/01 09:36:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 09:36:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 09:36:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 09:36:11 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 09:36:11 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/02/01 09:36:11 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/01 09:36:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local806998882_0001\n",
      "16/02/01 09:36:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/secondary_sort_map.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454348171875/secondary_sort_map.py\n",
      "16/02/01 09:36:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/secondary_sort_reduce.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454348171876/secondary_sort_reduce.py\n",
      "16/02/01 09:36:12 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 09:36:12 INFO mapreduce.Job: Running job: job_local806998882_0001\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/1902:0+888978\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=2903/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 63107; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26188140(104752560); length = 26257/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=2903/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000000_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/1901:0+888190\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=2936/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 63794; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26188144(104752576); length = 26253/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=2936/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000001_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/otherYears.txt:0+1078\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=8/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 70; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=8/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000002_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000000_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@344c22e1\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=352321536, maxSingleShuffleLimit=88080384, mergeThreshold=232532224, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->4\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 76239 len: 76243 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 76239 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 76239, inMemoryMapOutputs.size() -> 3, commitMemory -> 4, usedMemory ->76243\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76227 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 76243 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 76243 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76227 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=6565/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000000\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=6565/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000000_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000000_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000001_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@625b6997\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 38 len: 42 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 38 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 38, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->40\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 40, usedMemory ->42\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 42 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000001\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000001_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000001_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000002_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34986bf\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 76924 len: 76928 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 76924 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 76924, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->76924\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 50 len: 54 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 50 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 50, inMemoryMapOutputs.size() -> 2, commitMemory -> 76924, usedMemory ->76974\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 76974, usedMemory ->76976\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 76951 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 76976 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 76976 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76960 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=6568/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000002\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=6568/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000002_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000002_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Job job_local806998882_0001 running in uber mode : false\n",
      "16/02/01 09:36:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Job job_local806998882_0001 completed successfully\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1273416\n",
      "\t\tFILE: Number of bytes written=3433695\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9779130\n",
      "\t\tHDFS: Number of bytes written=94\n",
      "\t\tHDFS: Number of read operations=72\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=18\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=13138\n",
      "\t\tMap output records=13136\n",
      "\t\tMap output bytes=126971\n",
      "\t\tMap output materialized bytes=153297\n",
      "\t\tInput split bytes=331\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11809\n",
      "\t\tReduce shuffle bytes=153297\n",
      "\t\tReduce input records=13136\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=26272\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=2905604096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTemperature\n",
      "\t\tMissing=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1778246\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/02/01 09:36:13 INFO streaming.StreamJob: Output directory: output-secondarysort-streaming\n"
     ]
    }
   ],
   "source": [
    "#To do a secondary sort in Streaming, we can take advantage of a couple of library classes\n",
    "#that Hadoop provides. Here’s the driver that we can use to do a secondary sort:\n",
    "!hdfs dfs -rm -r output-secondarysort-streaming\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,1nr\" \\\n",
    "    -files secondary_sort_map.py,secondary_sort_reduce.py \\\n",
    "    -input SecondarySort \\\n",
    "    -output output-secondarysort-streaming \\\n",
    "    -mapper secondary_sort_map.py \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -reducer secondary_sort_reduce.py \\\n",
    "    -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:25:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 09:24 output-secondarysort-streaming/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup          9 2016-02-01 09:24 output-secondarysort-streaming/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup         18 2016-02-01 09:24 output-secondarysort-streaming/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         27 2016-02-01 09:24 output-secondarysort-streaming/part-00002\n",
      "16/02/01 09:25:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1902\t244\n",
      "1903\t-78\n",
      "1906\t-72\n",
      "1901\t317\n",
      "1904\t-72\n",
      "1907\t-72\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls output-secondarysort-streaming/*\n",
    "!hdfs dfs -cat output-secondarysort-streaming/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/30 09:12:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 09:13:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#stop hadoop/yarn cluster on my local machine (in this case)\n",
    "#!alias hstop=\"/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\"\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chris Caldwell Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chris Caldwell Homework\n",
    "# make a work directory on my local machine\n",
    "!mkdir ChrisC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#getting random ints for indexes\n",
    "indices = random.sample(range(1, 100), 10)\n",
    "\n",
    "#writing out the file\n",
    "intIndFile = open('ChrisC/hw2_1.txt', 'w')\n",
    "#for x in indices:\n",
    "#intIndFile.write(\"{}\\t\".format(x))\n",
    "intIndFile.write(\"\\t\\n\".join(str(x) for x in indices)+'\\t')\n",
    "intIndFile.close()\n",
    "\n",
    "#delete if previous existing file\n",
    "#!hdfs dfs -rm  /user/ubuntu/hw2_1.txt\n",
    "#moving file to hdfs\n",
    "#!hdfs dfs -put hw2_1.txt /user/ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\t\r\n",
      "62\t\r\n",
      "64\t\r\n",
      "59\t\r\n",
      "27\t\r\n",
      "41\t\r\n",
      "18\t\r\n",
      "19\t\r\n",
      "90\t\r\n",
      "94\t"
     ]
    }
   ],
   "source": [
    "!head ChrisC/hw2_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ChrisC/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChrisC/mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Input from standard in\n",
    "for ent in sys.stdin:\n",
    "    print(ent.strip())\n",
    "    # split entity into key / value\n",
    "    #key, val = ent.split('\\t')    \n",
    "    #print(\"{}\\t{}\".format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ChrisC/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ChrisC/reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "#print(\"109\\t\")\n",
    "#Input from standard in\n",
    "for ent in sys.stdin:\n",
    "    print(ent.strip())\n",
    "    # split entity into key / value\n",
    "    #key, val = ent.split('\\t')    \n",
    "    #print(\"{}\\t{}\".format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x ChrisC/mapper.py\n",
    "!chmod a+x ChrisC/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n",
      "3\r\n",
      "5\r\n",
      "8\r\n",
      "9\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"1\\t\\n5\\t\\n3\\t\\n8\\t\\n9\\t\" | python ChrisC/mapper.py |sort -k1,1 | python ChrisC/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 19:23:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 19:23:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Make directory on HDFS and copy the data file up there\n",
    "!hdfs dfs -mkdir ChrisC\n",
    "!hdfs dfs -put ChrisC/hw2_1.txt ChrisC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 19:36:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 19:36:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted hw2_1_out\n",
      "16/05/28 19:36:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 19:36:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/05/28 19:36:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/05/28 19:36:10 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/05/28 19:36:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 19:36:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/05/28 19:36:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local621821506_0001\n",
      "16/05/28 19:36:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/05/28 19:36:11 INFO mapreduce.Job: Running job: job_local621821506_0001\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Starting task: attempt_local621821506_0001_m_000000_0\n",
      "16/05/28 19:36:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/28 19:36:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ChrisC/hw2_1.txt:0+39\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ChrisC/mapper.py]\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: \n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Starting flush of map output\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Spilling map output\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: bufstart = 0; bufend = 40; bufvoid = 104857600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
      "16/05/28 19:36:11 INFO mapred.MapTask: Finished spill 0\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task:attempt_local621821506_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task 'attempt_local621821506_0001_m_000000_0' done.\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local621821506_0001_m_000000_0\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Starting task: attempt_local621821506_0001_r_000000_0\n",
      "16/05/28 19:36:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/05/28 19:36:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/05/28 19:36:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@150b04f3\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/05/28 19:36:11 INFO reduce.EventFetcher: attempt_local621821506_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/05/28 19:36:11 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local621821506_0001_m_000000_0 decomp: 62 len: 66 to MEMORY\n",
      "16/05/28 19:36:11 INFO reduce.InMemoryMapOutput: Read 62 bytes from map-output for attempt_local621821506_0001_m_000000_0\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 62, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->62\n",
      "16/05/28 19:36:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 62 bytes to disk to satisfy reduce memory limit\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: Merging 1 files, 66 bytes from disk\n",
      "16/05/28 19:36:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/05/28 19:36:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ChrisC/reducer.py]\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/05/28 19:36:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: Records R/W=10/1\n",
      "16/05/28 19:36:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task:attempt_local621821506_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task attempt_local621821506_0001_r_000000_0 is allowed to commit now\n",
      "16/05/28 19:36:11 INFO output.FileOutputCommitter: Saved output of task 'attempt_local621821506_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/hw2_1_out/_temporary/0/task_local621821506_0001_r_000000\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Records R/W=10/1 > reduce\n",
      "16/05/28 19:36:11 INFO mapred.Task: Task 'attempt_local621821506_0001_r_000000_0' done.\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local621821506_0001_r_000000_0\n",
      "16/05/28 19:36:11 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/05/28 19:36:12 INFO mapreduce.Job: Job job_local621821506_0001 running in uber mode : false\n",
      "16/05/28 19:36:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 19:36:12 INFO mapreduce.Job: Job job_local621821506_0001 completed successfully\n",
      "16/05/28 19:36:12 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210436\n",
      "\t\tFILE: Number of bytes written=749278\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=78\n",
      "\t\tHDFS: Number of bytes written=40\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=40\n",
      "\t\tMap output materialized bytes=66\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=66\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=726663168\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40\n",
      "16/05/28 19:36:12 INFO streaming.StreamJob: Output directory: hw2_1_out\n"
     ]
    }
   ],
   "source": [
    "#!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "!hdfs dfs -rm -r hw2_1_out\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper ChrisC/mapper.py \\\n",
    "   -reducer ChrisC/reducer.py \\\n",
    "   -input ChrisC/hw2_1.txt -output hw2_1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 19:35:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18\t\n",
      "19\t\n",
      "27\t\n",
      "41\t\n",
      "59\t\n",
      "62\t\n",
      "64\t\n",
      "77\t\n",
      "90\t\n",
      "94\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat hw2_1_out/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
